= Investigating popular datasets and DNN architectures
Dillon Sahadevan <dillon.sahadevan@ucalgary.ca>
:toc:

== Introduction

This paper is a continuation of "A Survey on Deep Neural Network Security in an Embedded Context". In this paper, we will be investigating some of the datasets and models described in survey. This paper will cover environment setup, dataset preparation, model training, and model evaluation.

== Datasets and Models

The following table describes the datasets and models that will be investigated in this paper.

[cols="1,1", options="header"]
|===
| Dataset | Model
| MNIST | CNN with 2 convolutional layers and 2 fully connected layers (CNN-2-2)
| CIFAR-10 | -
| ImageNet | -
|===

== Environment Setup

=== Hardware

The following table describes the hardware used for this paper.

[cols="1,1", options="header"]
|===
| Component | Specification
| CPU | Ryzen 5 5600
| GPU | NVIDIA GeForce RTX 3070
| RAM | 32 GB
|===

=== Software

The following table describes the relevant software used for this paper.

[cols="1,1", options="header"]
|===
| Component | Specification
| OS | Windows 11
| Python | 3.8.5
| PyTorch | 1.8.1
| CUDA | 11.1
|===

=== Setting up the Environment

Python was installed from the official https://www.python.org/downloads/[Python website]. PyTorch was installed using the following command (found on the https://pytorch.org/get-started/locally/[PyTorch website]):

```bash
pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116
```

=== Project Structure

The source code for this paper can be found on https://github.com/Dillonsd/ENEL592Assignment2[Github]. The directory structure is split into two parts: the 'models' directory and the 'datasets' directory. The 'models' directory contains the source code for the models that will be trained and evaluated. The 'datasets' directory contains the different datasets used.

For example, the following shows files relevant to MNIST and the CNN-2-2 model.

[plantuml, format=svg, opts="inline"]
----
skinparam Legend {
	BackgroundColor transparent
	BorderThickness 0
	FontName "Noto Serif", "DejaVu Serif", serif
	FontSize 17
}

' We use skinparams to hide our dummy participant
' and make it as little space as possible
skinparam SequenceLifeLineBorderThickness 0
skinparam SequenceLifeLineBorderColor transparent
skinparam SequenceParticipant {
	BackgroundColor transparent
	BorderColor transparent
	Shadowing false
	FontSize 0
	BorderThickness 0
	Padding 0
}
hide footbox

' "participant" nudges PlantUML to a sequence diagram
participant dummy

legend top left
Root
|_ Element 1
  |_ Element 1.1
  |_ Element 1.2
|_ Element 2
  |_ Element 2.1
end legend
----
